{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googletrans==3.1.0a0\n",
    "# !pip install sinling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sinling import SinhalaStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chamal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining Sinhala stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sinhala = [\"‡∑É‡∑Ñ\", \"‡∑É‡∂∏‡∂ú\", \"‡∑É‡∂∏‡∂ü\", \"‡∂Ö‡∑Ñ‡∑è\", \"‡∂Ü‡∑Ñ‡∑ä\", \"‡∂Ü\", \"‡∂ï‡∑Ñ‡∑ù\", \"‡∂Ö‡∂±‡∑ö\", \"‡∂Ö‡∂≥‡∑ù\", \"‡∂Ö‡∂¥‡∑ú‡∂∫‡∑í\", \"‡∂Ö‡∂¥‡∑ù\", \"‡∂Ö‡∂∫‡∑í‡∂∫‡∑ù\", \"‡∂Ü‡∂∫‡∑í\", \"‡∂å‡∂∫‡∑í\", \"‡∂†‡∑ì\", \"‡∂†‡∑í‡∑Ñ‡∑ä\", \"‡∂†‡∑í‡∂ö‡∑ä\", \"‡∑Ñ‡∑ù‚Äç\", \"‡∂Ø‡∑ù\", \"‡∂Ø‡∑ù‡∑Ñ‡∑ù\", \"‡∂∏‡∑ô‡∂±‡∑ä\", \"‡∑É‡∑ö\", \"‡∑Ä‡∑ê‡∂±‡∑í\", \"‡∂∂‡∂≥‡∑î\", \"‡∑Ä‡∂±‡∑ä\", \"‡∂Ö‡∂∫‡∑î‡∂ª‡∑î\", \"‡∂Ö‡∂∫‡∑î‡∂ª‡∑í‡∂±‡∑ä\", \"‡∂Ω‡∑ô‡∑É\", \"‡∑Ä‡∑ê‡∂©‡∑í\", \"‡∑Å‡∑ä‚Äç‡∂ª‡∑ì\", \"‡∑Ñ‡∑è\", \"‡∂∫\", \"‡∂±‡∑í‡∑É‡∑è\", \"‡∂±‡∑í‡∑É‡∑è‡∑Ä‡∑ô‡∂±‡∑ä\", \"‡∂∂‡∑Ä‡∂ß\", \"‡∂∂‡∑Ä\", \"‡∂∂‡∑Ä‡∑ô‡∂±‡∑ä\", \"‡∂±‡∂∏‡∑ä\", \"‡∑Ä‡∑ê‡∂©‡∑í\", \"‡∑É‡∑í‡∂ß\", \"‡∂Ø‡∑ì\", \"‡∂∏‡∑Ñ‡∑è\", \"‡∂∏‡∑Ñ\", \"‡∂¥‡∂∏‡∂´\", \"‡∂¥‡∂∏‡∂´‡∑í‡∂±‡∑ä\", \"‡∂¥‡∂∏‡∂±\", \"‡∑Ä‡∂±\", \"‡∑Ä‡∑í‡∂ß\", \"‡∑Ä‡∑í‡∂ß‡∑í‡∂±‡∑ä\", \"‡∂∏‡∑ö\", \"‡∂∏‡∑ô‡∂Ω‡∑ô‡∑É\", \"‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä\", \"‡∂á‡∂≠‡∑í\", \"‡∂Ω‡∑ô‡∑É\", \"‡∑É‡∑í‡∂Ø‡∑î\", \"‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä\", \"‡∂∫‡∂±\", \"‡∑É‡∂≥‡∑Ñ‡∑è\", \"‡∂∏‡∂ú‡∑í‡∂±‡∑ä\", \"‡∑Ñ‡∑ù‚Äç\", \"‡∂â‡∂≠‡∑è\", \"‡∂í\", \"‡∂ë‡∂∏\", \"‡∂Ø\", \"‡∂Ö‡∂≠‡∂ª\", \"‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑ä\", \"‡∑É‡∂∏‡∂ú\", \"‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥‡∑Ä\", \"‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥\", \"‡∂≠‡∑î‡∑Ö\", \"‡∂∂‡∑Ä\", \"‡∑Ä‡∑ê‡∂±‡∑í\", \"‡∂∏‡∑Ñ\", \"‡∂∏‡∑ô‡∂∏\", \"‡∂∏‡∑ô‡∑Ñ‡∑í\", \"‡∂∏‡∑ö\", \"‡∑Ä‡∑ô‡∂≠\", \"‡∑Ä‡∑ô‡∂≠‡∑í‡∂±‡∑ä\", \"‡∑Ä‡∑ô‡∂≠‡∂ß\", \"‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä\", \"‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∂ß\", \"‡∑Ä‡∑ô‡∂±\", \"‡∂ú‡∑ê‡∂±\", \"‡∂±‡∑ë\", \"‡∂Ö‡∂±‡∑î‡∑Ä\", \"‡∂±‡∑Ä\", \"‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥\", \"‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç\", \"‡∂Ø‡∑ê‡∂±‡∂ß\", \"‡∂ë‡∑Ñ‡∑ô‡∂±‡∑ä\", \"‡∂∏‡∑ô‡∑Ñ‡∑ô‡∂±‡∑ä\", \"‡∂ë‡∑Ñ‡∑ö\", \"‡∂∏‡∑ô‡∑Ñ‡∑ö\", \"‡∂∏\", \"‡∂≠‡∑Ä‡∂≠‡∑ä\", \"‡∂≠‡∑Ä \", \"‡∑É‡∑Ñ\",\n",
    "                     \"‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è\", \"‡∂ß\", \"‡∂ú‡∑ö\", \"‡∂ë\", \"‡∂ö\", \"‡∂ö‡∑ä\", \"‡∂∂‡∑Ä‡∂≠‡∑ä\", \"‡∂∂‡∑Ä‡∂Ø\", \"‡∂∏‡∂≠\", \"‡∂á‡∂≠‡∑î‡∂Ω‡∑î\", \"‡∂á‡∂≠‡∑î‡∑Ö‡∑î\", \"‡∂∏‡∑ô‡∑É‡∑ö\", \"‡∑Ä‡∂©‡∑è\", \"‡∑Ä‡∂©‡∑è‡∂≠‡∑ä‡∂∏\", \"‡∂±‡∑í‡∂≠‡∑í\", \"‡∂±‡∑í‡∂≠‡∑í‡∂≠‡∑ä\", \"‡∂±‡∑í‡∂≠‡∑ú‡∂ª\", \"‡∂±‡∑í‡∂≠‡∂ª\", \"‡∂â‡∂ö‡∑ä‡∂∂‡∑í‡∂≠‡∑í\", \"‡∂Ø‡∑ê‡∂±‡∑ä\", \"‡∂∫‡∂Ω‡∑í\", \"‡∂¥‡∑î‡∂±\", \"‡∂â‡∂≠‡∑í‡∂±‡∑ä\", \"‡∑É‡∑í‡∂ß\", \"‡∑É‡∑í‡∂ß‡∂±‡∑ä\", \"‡∂¥‡∂ß‡∂±‡∑ä\", \"‡∂≠‡∑ô‡∂ö‡∑ä\", \"‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è\", \"‡∑É‡∑è\", \"‡∂≠‡∑è‡∂ö‡∑ä\", \"‡∂≠‡∑î‡∑Ä‡∂ö‡∑ä\", \"‡∂¥‡∑Ä‡∑è\", \"‡∂Ø\", \"‡∑Ñ‡∑ù‚Äç\", \"‡∑Ä‡∂≠‡∑ä\", \"‡∑Ä‡∑í‡∂±‡∑è\", \"‡∑Ñ‡∑ê‡∂ª\", \"‡∂∏‡∑í‡∑É\", \"‡∂∏‡∑î‡∂≠‡∑ä\", \"‡∂ö‡∑í‡∂∏\", \"‡∂ö‡∑í‡∂∏‡∑ä\", \"‡∂á‡∂∫‡∑í\", \"‡∂∏‡∂±‡∑ä‡∂Ø\", \"‡∑Ñ‡∑ô‡∑Ä‡∂≠‡∑ä\", \"‡∂±‡∑ú‡∑Ñ‡∑ú‡∂≠‡∑ä\", \"‡∂¥‡∂≠‡∑è\", \"‡∂¥‡∑è‡∑É‡∑è\", \"‡∂ú‡∑è‡∂±‡∑ô\", \"‡∂≠‡∑Ä\", \"‡∂â‡∂≠‡∑è\", \"‡∂∂‡∑ú‡∑Ñ‡∑ù\", \"‡∑Ä‡∑Ñ‡∑è\", \"‡∑É‡∑ô‡∂Ø\", \"‡∑É‡∑ê‡∂±‡∑í‡∂±‡∑ä\", \"‡∑Ñ‡∂±‡∑í‡∂ö\", \"‡∂ë‡∂∏‡∑ä‡∂∂‡∑è\", \"‡∂ë‡∂∏‡∑ä‡∂∂‡∂Ω\", \"‡∂∂‡∑ú‡∂Ω\", \"‡∂±‡∂∏‡∑ä\", \"‡∑Ä‡∂±‡∑è‡∑Ñ‡∑í\", \"‡∂ö‡∂Ω‡∑ì\", \"‡∂â‡∂≥‡∑î‡∂ª‡∑è\", \"‡∂Ö‡∂±‡∑ä‡∂±\", \"‡∂î‡∂±‡∑ä‡∂±\", \"‡∂∏‡∑ô‡∂±‡∑ä‡∂±\", \"‡∂ã‡∂Ø‡∑ô‡∑É‡∑è\", \"‡∂¥‡∑í‡∂´‡∑í‡∑É\", \"‡∑É‡∂≥‡∑Ñ‡∑è\", \"‡∂Ö‡∂ª‡∂∂‡∂∫‡∑è\", \"‡∂±‡∑í‡∑É‡∑è\", \"‡∂ë‡∂±‡∑í‡∑É‡∑è\", \"‡∂ë‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä\", \"‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä\", \"‡∑Ñ‡∑ô‡∂∫‡∑í‡∂±‡∑ä\", \"‡∑É‡∑ö‡∂ö‡∑ä\", \"‡∑É‡∑ö‡∂ö\", \"‡∂ú‡∑ê‡∂±\", \"‡∂Ö‡∂±‡∑î‡∑Ä\", \"‡∂¥‡∂ª‡∑í‡∂Ø‡∑í\", \"‡∑Ä‡∑í‡∂ß\", \"‡∂≠‡∑ô‡∂ö‡∑ä\", \"‡∂∏‡∑ô‡∂≠‡∑ô‡∂ö‡∑ä\", \"‡∂∏‡∑ö‡∂≠‡∑è‡∂ö‡∑ä\", \"‡∂≠‡∑î‡∂ª‡∑î\", \"‡∂≠‡∑î‡∂ª‡∑è\", \"‡∂≠‡∑î‡∂ª‡∑è‡∑Ä‡∂ß\", \"‡∂≠‡∑î‡∂Ω‡∑í‡∂±‡∑ä\", \"‡∂±‡∂∏‡∑î‡∂≠‡∑ä\", \"‡∂ë‡∂±‡∂∏‡∑î‡∂≠‡∑ä\", \"‡∑Ä‡∑É‡∑ä\", \"‡∂∏‡∑ô‡∂±‡∑ä\", \"‡∂Ω‡∑ô‡∑É\", \"‡∂¥‡∂ª‡∑í‡∂Ø‡∑í\", \"‡∂ë‡∑Ñ‡∑ô‡∂≠‡∑ä\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Impact</th>\n",
       "      <th>StateLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∂∏‡∂∏ ‡∂±‡∑î‡∑Ä‡∂ª‡∂ë‡∑Ö‡∑í‡∂∫‡∑ö ‡∂â‡∂Ø‡∂±‡∑ä ‡∂ö‡∑ú‡∑Ö‡∂π‡∂ß ‡∂á‡∑Ä‡∑í‡∂Ω‡∑ä‡∂Ω‡∑è ‡∂ú‡∑è‡∂∫‡∂ö‡∂∫‡∑ô‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä‡∂±...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∂Ö‡∂Ø ‡∂ã‡∂Ø‡∑ö ‡∂Ø‡∑ê‡∂ö‡∂¥‡∑î ‡∑É‡∑î‡∂±‡∑ä‡∂Ø‡∂ª ‡∂Ø‡∂ª‡∑ä‡∑Å‡∂´‡∂∫‡∂ö‡∑ä. ‡∂±‡∑î‡∂ú‡∑ö‡∂ú‡∑ú‡∂© St.Johns...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smoking is a bad habit \\nMenda , Daneeüèè‚úå.\\nCri...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∑Ñ‡∑ô‡∑ÖPay for Business\\r\\niOS App ‡∂ë‡∂ö ‡∂∏‡∑ö ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂ß Ap...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shooting ‡∑Ä‡∂Ω‡∂ß ‡∂±‡∑î‡∑Ä‡∂ª ‡∂ú‡∑í‡∂∫‡∂¥‡∑î ‡∂∏‡∂ú‡∑ö ‡∑Ñ‡∑í‡∂≠ ‡∂ú‡∑í‡∂∫ ‡∂≠‡∑ê‡∂±‡∂ö‡∑ä.. ‚ù§Ô∏è...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‡∂∏‡∑ö ‡∂Ω‡∑É‡∑ä‡∑É‡∂± ‡∂Ö‡∑Ñ‡∑í‡∂Ç‡∑É‡∂ö ‡∂∏‡∑î‡∑Ñ‡∑î‡∂±‡∑î ‡∑Ä‡∂Ω‡∂ß ‡∑Ñ‡∑í‡∂±‡∑è‡∑Ä‡∂ö‡∑ä ‡∂ú‡∑ö‡∂±‡∑ä‡∂± ‡∂Ö‡∂¥‡∑í‡∂ß ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‡∑Ñ‡∑ô‡∂ß ‡∂Ø‡∑í‡∂±‡∂∫‡∑ö ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∑Ä‡∂± ‡∂Ü‡∑É‡∑í‡∂∫‡∑è‡∂±‡∑î ‡∂ö‡∑î‡∑É‡∂Ω‡∑è‡∂± ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂ö‡∂ß‡∑ä ‡∂≠‡∂ª‡∂ü...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‡∂Ö‡∂ª‡∂ú‡∂Ω‡∂∫‡∑ö ‡∂±‡∑í‡∂∫‡∂∏‡∑î‡∑Ä‡∂±‡∑ä ‡∂Ø‡∂©‡∂∫‡∂∏ ‡∂±‡∑Ä‡∂≠‡∂±‡∑î !! \\nLive video ‡∂ë‡∂ö‡∂ö...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡∂Ö‡∂¥‡∑í ‡∂Ö‡∂¥‡∑ä‡∂¥‡∂†‡∑ä‡∂†‡∑í ‡∑Ä‡∂ß‡∑ö ‡∂Ü‡∂©‡∂∏‡∑ä‡∂∂‡∂ª‡∑ô‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∑Ä‡∂ú‡∑ö ‡∂ë‡∂∫‡∑è ‡∑É‡∑î‡∑Ä ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡∂∂‡∑ä‚Äç‡∂ª‡∑Ñ‡∑ä‡∂∏‡∑è‡∑É‡∑ä‡∂≠‡∑ä‚Äç‡∂ª ‡∂†‡∑í‡∂≠‡∑ä‚Äç‡∂ª‡∂¥‡∂ß‡∑í‡∂∫‡∑ö ‚Äú‡∂ö‡∑ö‡∑É‡∂ª‡∑í‡∂∫‡∑è‚Äù ‡∑É‡∑í‡∂Ç‡∂Ø‡∑î‡∑Ä‡∂ß c...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement    Impact  StateLength\n",
       "0  ‡∂∏‡∂∏ ‡∂±‡∑î‡∑Ä‡∂ª‡∂ë‡∑Ö‡∑í‡∂∫‡∑ö ‡∂â‡∂Ø‡∂±‡∑ä ‡∂ö‡∑ú‡∑Ö‡∂π‡∂ß ‡∂á‡∑Ä‡∑í‡∂Ω‡∑ä‡∂Ω‡∑è ‡∂ú‡∑è‡∂∫‡∂ö‡∂∫‡∑ô‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂±‡∑ä‡∂±...  Positive          403\n",
       "1  ‡∂Ö‡∂Ø ‡∂ã‡∂Ø‡∑ö ‡∂Ø‡∑ê‡∂ö‡∂¥‡∑î ‡∑É‡∑î‡∂±‡∑ä‡∂Ø‡∂ª ‡∂Ø‡∂ª‡∑ä‡∑Å‡∂´‡∂∫‡∂ö‡∑ä. ‡∂±‡∑î‡∂ú‡∑ö‡∂ú‡∑ú‡∂© St.Johns...  Positive          298\n",
       "2  Smoking is a bad habit \\nMenda , Daneeüèè‚úå.\\nCri...  Positive           91\n",
       "3  ‡∑Ñ‡∑ô‡∑ÖPay for Business\\r\\niOS App ‡∂ë‡∂ö ‡∂∏‡∑ö ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂ß Ap...  Positive          112\n",
       "4  Shooting ‡∑Ä‡∂Ω‡∂ß ‡∂±‡∑î‡∑Ä‡∂ª ‡∂ú‡∑í‡∂∫‡∂¥‡∑î ‡∂∏‡∂ú‡∑ö ‡∑Ñ‡∑í‡∂≠ ‡∂ú‡∑í‡∂∫ ‡∂≠‡∑ê‡∂±‡∂ö‡∑ä.. ‚ù§Ô∏è...  Positive          359\n",
       "5  ‡∂∏‡∑ö ‡∂Ω‡∑É‡∑ä‡∑É‡∂± ‡∂Ö‡∑Ñ‡∑í‡∂Ç‡∑É‡∂ö ‡∂∏‡∑î‡∑Ñ‡∑î‡∂±‡∑î ‡∑Ä‡∂Ω‡∂ß ‡∑Ñ‡∑í‡∂±‡∑è‡∑Ä‡∂ö‡∑ä ‡∂ú‡∑ö‡∂±‡∑ä‡∂± ‡∂Ö‡∂¥‡∑í‡∂ß ...  Positive          386\n",
       "6  ‡∑Ñ‡∑ô‡∂ß ‡∂Ø‡∑í‡∂±‡∂∫‡∑ö ‡∂Ü‡∂ª‡∂∏‡∑ä‡∂∑ ‡∑Ä‡∂± ‡∂Ü‡∑É‡∑í‡∂∫‡∑è‡∂±‡∑î ‡∂ö‡∑î‡∑É‡∂Ω‡∑è‡∂± ‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂ö‡∂ß‡∑ä ‡∂≠‡∂ª‡∂ü...  Positive          170\n",
       "7  ‡∂Ö‡∂ª‡∂ú‡∂Ω‡∂∫‡∑ö ‡∂±‡∑í‡∂∫‡∂∏‡∑î‡∑Ä‡∂±‡∑ä ‡∂Ø‡∂©‡∂∫‡∂∏ ‡∂±‡∑Ä‡∂≠‡∂±‡∑î !! \\nLive video ‡∂ë‡∂ö‡∂ö...  Negative          412\n",
       "8  ‡∂Ö‡∂¥‡∑í ‡∂Ö‡∂¥‡∑ä‡∂¥‡∂†‡∑ä‡∂†‡∑í ‡∑Ä‡∂ß‡∑ö ‡∂Ü‡∂©‡∂∏‡∑ä‡∂∂‡∂ª‡∑ô‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑Ä‡∑è ‡∑Ä‡∂ú‡∑ö ‡∂ë‡∂∫‡∑è ‡∑É‡∑î‡∑Ä ...  Negative          270\n",
       "9  ‡∂∂‡∑ä‚Äç‡∂ª‡∑Ñ‡∑ä‡∂∏‡∑è‡∑É‡∑ä‡∂≠‡∑ä‚Äç‡∂ª ‡∂†‡∑í‡∂≠‡∑ä‚Äç‡∂ª‡∂¥‡∂ß‡∑í‡∂∫‡∑ö ‚Äú‡∂ö‡∑ö‡∑É‡∂ª‡∑í‡∂∫‡∑è‚Äù ‡∑É‡∑í‡∂Ç‡∂Ø‡∑î‡∑Ä‡∂ß c...  Positive          362"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./PublicFigureStatementsSinglish.xls\", encoding=\"utf-16\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering dataset into positives and negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  517\n",
      "Number of negative tweets:  505\n"
     ]
    }
   ],
   "source": [
    "all_positive_tweets = df[df[\"Impact\"] ==\n",
    "                         \"Positive\"][\"Statement\"].values.tolist()\n",
    "all_negative_tweets = df[df[\"Impact\"] ==\n",
    "                         \"Negative\"][\"Statement\"].values.tolist()\n",
    "\n",
    "print(\"Number of positive tweets: \", len(all_positive_tweets))\n",
    "print(\"Number of negative tweets: \", len(all_negative_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomly choosing 300 samples for each positives and negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  300\n",
      "Number of negative tweets:  300\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 300\n",
    "all_positive_tweets = np.random.choice(\n",
    "    all_positive_tweets, size=dataset_size, replace=False).tolist()\n",
    "all_negative_tweets = np.random.choice(\n",
    "    all_negative_tweets, size=dataset_size, replace=False).tolist()\n",
    "\n",
    "\n",
    "print(\"Number of positive tweets: \", len(all_positive_tweets))\n",
    "print(\"Number of negative tweets: \", len(all_negative_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet processing function depending on the language to translate to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, language=\"si\"):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "        language: language to translate to \"si\" or \"en\"\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "\n",
    "    english_stemmer = PorterStemmer()\n",
    "    sinhala_stemmer = SinhalaStemmer()\n",
    "\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r\"\\$\\w*\", \"\", tweet)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"(http|https|ftp):\\/\\/(\\S*)\", \"\", tweet)\n",
    "\n",
    "    # remove hashtag sign from words\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    translator = Translator()\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    def filter_english(\n",
    "        word): return word not in stopwords_english and word not in string.punctuation\n",
    "\n",
    "    def filter_sinhala(\n",
    "        word): return word not in stopwords_sinhala and word not in string.punctuation\n",
    "\n",
    "    def stem_english(word):\n",
    "        if (filter_english(word)):\n",
    "            stem_word = english_stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    def stem_sinhala(word):\n",
    "        if (filter_sinhala(word)):\n",
    "            stem_word = sinhala_stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word[0])\n",
    "\n",
    "    if language == \"en\":\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            detected_language = translator.detect(word)\n",
    "            if \"en\" in detected_language.lang:  # parse English word\n",
    "                stem_english(word)\n",
    "            elif \"si\" in detected_language.lang:  # parse Sinhala word\n",
    "                if filter_sinhala(word):\n",
    "                    translated_text = translator.translate(word)\n",
    "                    tokenized_text = tokenizer.tokenize(translated_text.text)\n",
    "                    for word in tokenized_text:\n",
    "                        stem_english(word)\n",
    "\n",
    "    elif language == \"si\":\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            detected_language = translator.detect(word)\n",
    "            if \"en\" in detected_language.lang:  # parse English word\n",
    "                if (filter_english(word)):\n",
    "                    translated_text = translator.translate(word)\n",
    "                    tokenized_text = tokenizer.tokenize(translated_text.text)\n",
    "                    for word in tokenized_text:\n",
    "                        stem_sinhala(word)\n",
    "            elif \"si\" in detected_language.lang:  # parse Sinhala word\n",
    "                stem_sinhala(word)\n",
    "\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word frequency dictionary `(word, label): frequency`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate tweets and create labels accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))),\n",
    "                   np.zeros((len(all_negative_tweets))))\n",
    "\n",
    "freqs = build_freqs(tweets, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide train and test split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    tweets, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency lookup helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "\n",
    "    n = freqs.get((word, label), 0)\n",
    "\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive bayes model training function returning the logprior and loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation.\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos += 1\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs, word, 1)\n",
    "        freq_neg = lookup(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01666705248521172\n",
      "4441\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "\n",
    "print(logprior)\n",
    "print(len(loglikelihood))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(sample):\n",
    "    y_hats = []\n",
    "    for tweet in sample:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "    return y_hats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shopify ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è ‡∂ö‡∂ª‡∂± ‡∂Ø‡∑ö‡∑Å‡∑ì‡∂∫ eCommerce ‡∑Ä‡∑ä‚Äç‡∂∫‡∑è‡∂¥‡∑è‡∂ª ‡∑É‡∂≥...</td>\n",
       "      <td>negative</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good Morning Everyone \\n‡∂î‡∂∫‡∑è‡∂Ω ‡∂∂‡∂Ω‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑ô ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑ê‡∂Ø...</td>\n",
       "      <td>negative</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Independent Televison Network Itn ‡∂ë‡∂ö‡∑ö ‡∂∂‡∑î‡∂Ø‡∑ä‡∂∞‡∑í‡∂∏‡∂≠...</td>\n",
       "      <td>positive</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∂á‡∂≠‡∑ä‡∂≠‡∂ß‡∂∏ ‡∂î‡∂∫ ‡∂∫‡∂ß‡∂≠‡∑ä ‡∂ö‡∂ª‡∂ú‡∂≠‡∑ä‡∂≠‡∑è ‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä‡∂ß ‡∂í‡∂ö ‡∂í ‡∂ö‡∑è‡∂Ω‡∑ô Upper ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>‚ùå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∂ö‡∑ú‡∑Ñ‡∑ö ‡∂¥‡∑í‡∂¥‡∑î‡∂´‡∂≠‡∑ä ‡∂∏‡∂Ω‡∑ä ‡∂ë‡∂ö‡∑Ä‡∂ú‡∑ö‡∂∏ ‡∑É‡∑î‡∑Ä‡∂≥‡∂∫‡∑ì.‡∑Ñ‡∂ª‡∑í‡∂∏ ‡∂Ω‡∑É‡∑ä‡∑É‡∂±‡∂ß ‡∂ú‡∑è‡∂∫...</td>\n",
       "      <td>positive</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement    Impact Result\n",
       "0  Shopify ‡∂∑‡∑è‡∑Ä‡∑í‡∂≠‡∑è ‡∂ö‡∂ª‡∂± ‡∂Ø‡∑ö‡∑Å‡∑ì‡∂∫ eCommerce ‡∑Ä‡∑ä‚Äç‡∂∫‡∑è‡∂¥‡∑è‡∂ª ‡∑É‡∂≥...  negative      ‚ùå\n",
       "1  Good Morning Everyone \\n‡∂î‡∂∫‡∑è‡∂Ω ‡∂∂‡∂Ω‡∂±‡∑ä ‡∂â‡∂±‡∑ä‡∂±‡∑ô ‡∂Ö‡∑Ä‡∑î‡∂ª‡∑ê‡∂Ø...  negative      ‚úÖ\n",
       "2  Independent Televison Network Itn ‡∂ë‡∂ö‡∑ö ‡∂∂‡∑î‡∂Ø‡∑ä‡∂∞‡∑í‡∂∏‡∂≠...  positive      ‚ùå\n",
       "3  ‡∂á‡∂≠‡∑ä‡∂≠‡∂ß‡∂∏ ‡∂î‡∂∫ ‡∂∫‡∂ß‡∂≠‡∑ä ‡∂ö‡∂ª‡∂ú‡∂≠‡∑ä‡∂≠‡∑è ‡∂ö‡∑í‡∑Ä‡∑ä‡∑Ä‡∂ß ‡∂í‡∂ö ‡∂í ‡∂ö‡∑è‡∂Ω‡∑ô Upper ...  negative      ‚ùå\n",
       "4  ‡∂ö‡∑ú‡∑Ñ‡∑ö ‡∂¥‡∑í‡∂¥‡∑î‡∂´‡∂≠‡∑ä ‡∂∏‡∂Ω‡∑ä ‡∂ë‡∂ö‡∑Ä‡∂ú‡∑ö‡∂∏ ‡∑É‡∑î‡∑Ä‡∂≥‡∂∫‡∑ì.‡∑Ñ‡∂ª‡∑í‡∂∏ ‡∂Ω‡∑É‡∑ä‡∑É‡∂±‡∂ß ‡∂ú‡∑è‡∂∫...  positive      ‚úÖ"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_indices = np.random.randint(0, len(test_x) - 1, 5)\n",
    "y_hats = predictions(np.take(test_x, random_indices))\n",
    "print(\"Outputs\")\n",
    "data = []\n",
    "for i in range(5):\n",
    "    data.append([test_x[i], \"positive\" if y_hats[i] >\n",
    "                0 else \"negative\", \"‚úÖ\" if y_hats[i] - test_y[i] == 0 else \"‚ùå\"])\n",
    "pd.DataFrame(data, columns=[\"Statement\", \"Impact\", \"Result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = predictions(test_x)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(test_y - y_hats))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9833\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1041ae8ea6e6e9df31131924fedcf13e5c3e7b3121aae097522c388ea1968a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
