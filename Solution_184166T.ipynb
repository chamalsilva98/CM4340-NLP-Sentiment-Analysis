{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install googletrans == 3.1.0a0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sinling import SinhalaStemmer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining Sinhala stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sinhala = [\"සහ\", \"සමග\", \"සමඟ\", \"අහා\", \"ආහ්\", \"ආ\", \"ඕහෝ\", \"අනේ\", \"අඳෝ\", \"අපොයි\", \"අපෝ\", \"අයියෝ\", \"ආයි\", \"ඌයි\", \"චී\", \"චිහ්\", \"චික්\", \"හෝ‍\", \"දෝ\", \"දෝහෝ\", \"මෙන්\", \"සේ\", \"වැනි\", \"බඳු\", \"වන්\", \"අයුරු\", \"අයුරින්\", \"ලෙස\", \"වැඩි\", \"ශ්‍රී\", \"හා\", \"ය\", \"නිසා\", \"නිසාවෙන්\", \"බවට\", \"බව\", \"බවෙන්\", \"නම්\", \"වැඩි\", \"සිට\", \"දී\", \"මහා\", \"මහ\", \"පමණ\", \"පමණින්\", \"පමන\", \"වන\", \"විට\", \"විටින්\", \"මේ\", \"මෙලෙස\", \"මෙයින්\", \"ඇති\", \"ලෙස\", \"සිදු\", \"වශයෙන්\", \"යන\", \"සඳහා\", \"මගින්\", \"හෝ‍\", \"ඉතා\", \"ඒ\", \"එම\", \"ද\", \"අතර\", \"විසින්\", \"සමග\", \"පිළිබඳව\", \"පිළිබඳ\", \"තුළ\", \"බව\", \"වැනි\", \"මහ\", \"මෙම\", \"මෙහි\", \"මේ\", \"වෙත\", \"වෙතින්\", \"වෙතට\", \"වෙනුවෙන්\", \"වෙනුවට\", \"වෙන\", \"ගැන\", \"නෑ\", \"අනුව\", \"නව\", \"පිළිබඳ\", \"විශේෂ\", \"දැනට\", \"එහෙන්\", \"මෙහෙන්\", \"එහේ\", \"මෙහේ\", \"ම\", \"තවත්\", \"තව \", \"සහ\",\n",
    "                     \"දක්වා\", \"ට\", \"ගේ\", \"එ\", \"ක\", \"ක්\", \"බවත්\", \"බවද\", \"මත\", \"ඇතුලු\", \"ඇතුළු\", \"මෙසේ\", \"වඩා\", \"වඩාත්ම\", \"නිති\", \"නිතිත්\", \"නිතොර\", \"නිතර\", \"ඉක්බිති\", \"දැන්\", \"යලි\", \"පුන\", \"ඉතින්\", \"සිට\", \"සිටන්\", \"පටන්\", \"තෙක්\", \"දක්වා\", \"සා\", \"තාක්\", \"තුවක්\", \"පවා\", \"ද\", \"හෝ‍\", \"වත්\", \"විනා\", \"හැර\", \"මිස\", \"මුත්\", \"කිම\", \"කිම්\", \"ඇයි\", \"මන්ද\", \"හෙවත්\", \"නොහොත්\", \"පතා\", \"පාසා\", \"ගානෙ\", \"තව\", \"ඉතා\", \"බොහෝ\", \"වහා\", \"සෙද\", \"සැනින්\", \"හනික\", \"එම්බා\", \"එම්බල\", \"බොල\", \"නම්\", \"වනාහි\", \"කලී\", \"ඉඳුරා\", \"අන්න\", \"ඔන්න\", \"මෙන්න\", \"උදෙසා\", \"පිණිස\", \"සඳහා\", \"අරබයා\", \"නිසා\", \"එනිසා\", \"එබැවින්\", \"බැවින්\", \"හෙයින්\", \"සේක්\", \"සේක\", \"ගැන\", \"අනුව\", \"පරිදි\", \"විට\", \"තෙක්\", \"මෙතෙක්\", \"මේතාක්\", \"තුරු\", \"තුරා\", \"තුරාවට\", \"තුලින්\", \"නමුත්\", \"එනමුත්\", \"වස්\", \"මෙන්\", \"ලෙස\", \"පරිදි\", \"එහෙත්\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"./PublicFigureStatementsSinglish.xls\", encoding=\"utf-16\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering dataset into positives and negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = df[df[\"Impact\"] ==\n",
    "                         \"Positive\"][\"Statement\"].values.tolist()\n",
    "all_negative_tweets = df[df[\"Impact\"] ==\n",
    "                         \"Negative\"][\"Statement\"].values.tolist()\n",
    "\n",
    "print(\"Number of positive tweets: \", len(all_positive_tweets))\n",
    "print(\"Number of negative tweets: \", len(all_negative_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomly choosing 300 samples for each positives and negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 300\n",
    "all_positive_tweets = np.random.choice(\n",
    "    all_positive_tweets, size=dataset_size, replace=False).tolist()\n",
    "all_negative_tweets = np.random.choice(\n",
    "    all_negative_tweets, size=dataset_size, replace=False).tolist()\n",
    "\n",
    "\n",
    "print(\"Number of positive tweets: \", len(all_positive_tweets))\n",
    "print(\"Number of negative tweets: \", len(all_negative_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet processing function depending on the language to translate to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, language=\"si\"):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "        language: language to translate to \"si\" or \"en\"\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "\n",
    "    english_stemmer = PorterStemmer()\n",
    "    sinhala_stemmer = SinhalaStemmer()\n",
    "\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r\"\\$\\w*\", \"\", tweet)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r\"(http|https|ftp):\\/\\/(\\S*)\", \"\", tweet)\n",
    "\n",
    "    # remove hashtag sign from words\n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    translator = Translator()\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    def filter_english(\n",
    "        word): return word not in stopwords_english and word not in string.punctuation\n",
    "\n",
    "    def filter_sinhala(\n",
    "        word): return word not in stopwords_sinhala and word not in string.punctuation\n",
    "\n",
    "    def stem_english(word):\n",
    "        if (filter_english(word)):\n",
    "            stem_word = english_stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    def stem_sinhala(word):\n",
    "        if (filter_sinhala(word)):\n",
    "            stem_word = sinhala_stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word[0])\n",
    "\n",
    "    if language == \"en\":\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            detected_language = translator.detect(word)\n",
    "            if \"en\" in detected_language.lang:  # parse English word\n",
    "                stem_english(word)\n",
    "            elif \"si\" in detected_language.lang:  # parse Sinhala word\n",
    "                if filter_sinhala(word):\n",
    "                    translated_text = translator.translate(word)\n",
    "                    tokenized_text = tokenizer.tokenize(translated_text.text)\n",
    "                    for word in tokenized_text:\n",
    "                        stem_english(word)\n",
    "\n",
    "    elif language == \"si\":\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            detected_language = translator.detect(word)\n",
    "            if \"en\" in detected_language.lang:  # parse English word\n",
    "                if (filter_english(word)):\n",
    "                    translated_text = translator.translate(word)\n",
    "                    tokenized_text = tokenizer.tokenize(translated_text.text)\n",
    "                    for word in tokenized_text:\n",
    "                        stem_sinhala(word)\n",
    "            elif \"si\" in detected_language.lang:  # parse Sinhala word\n",
    "                stem_sinhala(word)\n",
    "\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word frequency dictionary `(word, label): frequency`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))),\n",
    "                   np.zeros((len(all_negative_tweets))))\n",
    "\n",
    "freqs = build_freqs(tweets, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0\n",
    "\n",
    "    n = freqs.get((word, label), 0)\n",
    "\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation.\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos += 1\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs, word, 1)\n",
    "        freq_neg = lookup(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "\n",
    "print(logprior)\n",
    "print(len(loglikelihood))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'බබා කෝලි නම් ගේමක් නෑ'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    sum = 0\n",
    "    for i, y in enumerate(test_y):\n",
    "        sum += np.abs(y - y_hats[i])\n",
    "    error = sum / len(test_y)\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" % (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b1041ae8ea6e6e9df31131924fedcf13e5c3e7b3121aae097522c388ea1968a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
